{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fa70af",
   "metadata": {},
   "source": [
    "### Theory About Reproducibility\n",
    "Reproducing or rather producing the quantized model is the deadliest pain the ass I have felt in a while. It was bad.\n",
    "\n",
    "Keeping the technicalities about SELECT_OPS and Flex Delegate etc aside, just confirm your setup as follow. I have already talked about using virtual environment as kernel in diskloader.ipynb\n",
    "\n",
    "Final setup\n",
    "- OS: Ubuntu Server  \n",
    "- Python: 3.12  \n",
    "- TensorFlow: 2.18.0 (regular, CPU-only)  \n",
    "  - Do NOT use `tensorflow[and-cuda]` (conversion fails).  \n",
    "  - Regular `tensorflow==2.18.0` works fine with Python 3.12.\n",
    "    ```bash\n",
    "    pip uninstall -y tensorflow[and-cuda]\n",
    "    pip install tensorflow==2.18.*\n",
    "    ````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dbbd92-606b-4491-a7cb-4a1154416213",
   "metadata": {},
   "source": [
    "### Prerequisites (uncomment to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9344e68-cb7f-41e2-8787-aa60a45010eb",
   "metadata": {},
   "source": [
    "#### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec797e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.18.1\n",
      "Uninstalling tensorflow-2.18.1:\n",
      "  Successfully uninstalled tensorflow-2.18.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow==2.18.*\n",
      "  Using cached tensorflow-2.18.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (1.74.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (3.11.2)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.4.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.18.*) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.*) (14.1.0)\n",
      "Requirement already satisfied: namex in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.*) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.*) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.*) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.*) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.*) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.*) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.*) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.*) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.*) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.*) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.*) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.*) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.*) (0.1.2)\n",
      "Using cached tensorflow-2.18.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.6 MB)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.18.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Assuming virtual environment .tens with python 3.12 is being used\n",
    "%pip uninstall -y tensorflow[and-cuda]\n",
    "%pip install tensorflow==2.18.* tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a51d2-53f4-4fcf-aa17-542fe9f0ae70",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "- No need to re-run if already ran in `diskloader.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a061a987-5ae6-465d-ad60-a61d3e60590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Download QuickDraw dataset\n",
    "# %python download_data.py\n",
    "\n",
    "# # Step 2: Combine stroke data across different partitions (train, val, test).\n",
    "# # The original data (e.g., airplanes.npz) is split into these partitions, \n",
    "# # which reduces the number of data points. We merge all partitions for a complete dataset.\n",
    "# %python combine_strokes.py\n",
    "\n",
    "# # Step 3: Convert each `.npy` and `.npz` file into individual sample files.\n",
    "# # For example, `airplanes.npy` contains (N, 784), where N is the number of samples \n",
    "# # and each sample is a flattened 28x28 image (784 features). We split this into\n",
    "# # individual `.npy` files like `airplanes/000001.npy`, `airplanes/000002.npy`, etc.\n",
    "# # This allows us to load specific slices of data (e.g., airplanes[3012:33012]) \n",
    "# # without having to load the entire file, reducing memory usage and avoiding memory eviction issues.\n",
    "# %python convert_to_individual_npys.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27283d0b-79c2-4618-90dd-4808a3056d59",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de3aac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 16:59:03.655685: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756054743.668878  352251 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756054743.672396  352251 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39058146",
   "metadata": {},
   "source": [
    "### Using the same dataloader pipeline as used in training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96186eed",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a119d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "LABELS_JSON    = \"/home/priyanka/doodle-vision/training/dataset/label_map.json\"\n",
    "PROCESSED_DATA_DIR = \"/home/priyanka/doodle-vision/training/dataset/processed\"\n",
    "\n",
    "# Data props\n",
    "NUM_CLASSES = 345\n",
    "\n",
    "# Training\n",
    "SAMPLES_PER_CLASS = 30_000\n",
    "SPLIT_RATIOS = (0.8, 0.1, 0.1) # train, val, test\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680d4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABELS_JSON, 'r') as f:\n",
    "    LABEL_MAP = json.load(f)\n",
    "    \n",
    "REV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc647847",
   "metadata": {},
   "source": [
    "### Disk Data Loader for Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b290af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dataset: 100%|███████████████████████████████████████████████████████████████| 345/345 [18:50<00:00,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In-Memory Mode] Loaded 10,350,000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the individual + in-memory mode for tflite inference\n",
    "images = []\n",
    "strokes = []\n",
    "\n",
    "# Always list of integers\n",
    "labels = []\n",
    "\n",
    "for cls, label in tqdm(LABEL_MAP.items(), desc=\"Building dataset\"):\n",
    "    image_files = sorted(\n",
    "        glob(os.path.join(PROCESSED_DATA_DIR, \"images\", cls, \"*.npy\"))\n",
    "    )\n",
    "    stroke_files = sorted(\n",
    "        glob(os.path.join(PROCESSED_DATA_DIR, \"strokes\", cls, \"*.npy\"))\n",
    "    )\n",
    "\n",
    "    N = min(len(image_files), len(stroke_files), SAMPLES_PER_CLASS)\n",
    "\n",
    "    for i in range(N):\n",
    "        image_path = image_files[i]\n",
    "        stroke_path = stroke_files[i]\n",
    "\n",
    "        images.append(np.load(image_path))\n",
    "        strokes.append(np.load(stroke_path))\n",
    "        labels.append(label)\n",
    "\n",
    "print(f\"[In-Memory Mode] Loaded {len(labels):,} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0353989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(images, strokes, labels):\n",
    "    for image, stroke, label in zip(images, strokes, labels):\n",
    "        yield (\n",
    "            image,\n",
    "            stroke,\n",
    "            tf.one_hot(label, depth=NUM_CLASSES),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c89c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sample(img, stroke, label):\n",
    "    return {\"stroke_input\": stroke, \"image_input\": img}, label\n",
    "\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(28, 28, 1), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(130, 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.int32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333179bd",
   "metadata": {},
   "source": [
    "### Train, Val, and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d449e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle once and split\n",
    "total = len(labels)\n",
    "indices = np.arange(total)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_end = int(SPLIT_RATIOS[0] * total)\n",
    "val_end = train_end + int(SPLIT_RATIOS[1] * total)\n",
    "\n",
    "train_idx = indices[:train_end]\n",
    "val_idx = indices[train_end:val_end]\n",
    "test_idx = indices[val_end:]\n",
    "\n",
    "# Train\n",
    "train_images =  [images[i] for i in train_idx]\n",
    "train_strokes = [strokes[i] for i in train_idx]\n",
    "train_labels =  [labels[i] for i in train_idx]\n",
    "\n",
    "# Val\n",
    "val_images =  [images[i] for i in val_idx]\n",
    "val_strokes = [strokes[i] for i in val_idx]\n",
    "val_labels =  [labels[i] for i in val_idx]\n",
    "\n",
    "# Test\n",
    "test_images =  [images[i] for i in test_idx]\n",
    "test_strokes = [strokes[i] for i in test_idx]\n",
    "test_labels =  [labels[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c75700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(images, strokes, labels, is_shuffle=False):\n",
    "    def gen():\n",
    "        return data_generator(images, strokes, labels)\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "    if is_shuffle:\n",
    "        ds = ds.shuffle(BATCH_SIZE * 10)\n",
    "\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.map(format_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afe788d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1756055885.711058  352251 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "test_ds = build_dataset(test_images, test_strokes, test_labels).take(math.ceil(len(test_labels) / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81e27e",
   "metadata": {},
   "source": [
    "### Quantizing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4ae3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original model\n",
    "model = tf.keras.models.load_model('/home/priyanka/doodle-vision/training/models/hybrid_345_classes_30000_examples.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b03f1f0a-c6c3-4821-af2a-73a2397432bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Keras Model Size: 14.86 MB\n",
      "Weights-Only Model Size: 14.84 MB\n"
     ]
    }
   ],
   "source": [
    "original_model_size = os.path.getsize(\n",
    "    '/home/priyanka/doodle-vision/training/models/hybrid_345_classes_30000_examples.keras'\n",
    ")\n",
    "print(f\"Original Keras Model Size: {original_model_size / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "\n",
    "weights_only_path = '/home/priyanka/doodle-vision/training/models/hybrid_345_classes_30000_examples.weights.h5'\n",
    "model.save_weights(weights_only_path)\n",
    "\n",
    "weights_size_bytes = os.path.getsize(weights_only_path)\n",
    "print(f\"Weights-Only Model Size: {weights_size_bytes / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e49f860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of the model's layers:\n",
      "Layer 'conv2d': float32\n",
      "Layer 'conv2d_1': float32\n",
      "Layer 'conv2d_2': float32\n",
      "Layer 'conv2d_3': float32\n",
      "Layer 'dense': float32\n",
      "Layer 'dense_1': float32\n",
      "Layer 'dense_2': float32\n",
      "Layer 'dense_3': float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision of the model's layers:\")\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, 'kernel'):\n",
    "        print(f\"Layer '{layer.name}': {layer.kernel.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb02cf",
   "metadata": {},
   "source": [
    "### 1. Dynamic Range Quantization\n",
    "This is the default quantization method. It quantizes the model's weights to 8-bit precision during conversion. This reduces model size but keeps activations in floating-point, offering a good balance between size reduction and minimal accuracy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2142eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpl5q5rvfe/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpl5q5rvfe/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpl5q5rvfe'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 130, 3), dtype=tf.float32, name='stroke_input'), TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='image_input')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 345), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  132505096533968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096535888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096535696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096534160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096533392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096535312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096535120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096528208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121395856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121393744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121392592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121394704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121395280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121392208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121392400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121394320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121396816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121397008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121396240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121395664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121398352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121398160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121399696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121395088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121397200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121398736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121396048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121398928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121396624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121401616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121403344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121399888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121399504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121402384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121403920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121401040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121402192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121402576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121403152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121404496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121403728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121405072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Dynamic range quantized model saved to best_model_345_c_30000_ex_dynamic_quant.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1756055889.713178  352251 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1756055889.713203  352251 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1756055889.735993  352251 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "def dynamic_range_quantization(keras_model, save_path):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"Dynamic range quantized model saved to {save_path}\")\n",
    "    return os.path.getsize(save_path)\n",
    "\n",
    "dynamic_range_size = dynamic_range_quantization(model, 'best_model_345_c_30000_ex_dynamic_quant.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8455c3",
   "metadata": {},
   "source": [
    "### 2. Float16 Quantization\n",
    "\n",
    "This method converts model weights to 16-bit floating-point numbers. It offers a 50% reduction in model size compared to the original 32-bit float model, with almost no loss in accuracy. This is a good middle ground if dynamic range or integer quantization leads to unacceptable accuracy drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37199965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpurhncox0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpurhncox0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpurhncox0'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 130, 3), dtype=tf.float32, name='stroke_input'), TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='image_input')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 345), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  132505096533968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096535888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096535696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096534160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096533392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096535312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096535120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505096528208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121395856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121393744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121392592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121394704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121395280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121392208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121392400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121394320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121396816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121397008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121396240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121395664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121398352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121398160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121399696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121395088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121397200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121398736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121396048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121398928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121396624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121401616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121403344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121399888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121399504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121402384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121403920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121401040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121402192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121402576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121403152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121404496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121403728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132505121405072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Float16 quantized model saved to best_model_345_c_30000_ex_fp16_quant.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1756055891.391189  352251 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1756055891.391202  352251 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    }
   ],
   "source": [
    "def float16_quantization(keras_model, save_path):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    tflite_model = converter.convert()\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"Float16 quantized model saved to {save_path}\")\n",
    "    return os.path.getsize(save_path)\n",
    "\n",
    "float16_size = float16_quantization(model, 'best_model_345_c_30000_ex_fp16_quant.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb479ca",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "531b8dae-80d4-4e54-b393-ecd01a0df390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tflite_model(model_path, test_dataset):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Find the indices of the 'image_input' and 'stroke_input'\n",
    "    image_idx = [i for i, d in enumerate(input_details) if \"image_input\" in d[\"name\"]][0]\n",
    "    stroke_idx = [i for i, d in enumerate(input_details) if \"stroke_input\" in d[\"name\"]][0]\n",
    "    output_idx = 0\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(test_dataset, desc=f\"Evaluating {os.path.basename(model_path)}\", dynamic_ncols=True)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        inputs, labels = batch\n",
    "        images = inputs[\"image_input\"].numpy()\n",
    "        strokes = inputs[\"stroke_input\"].numpy()\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            # Set input tensors\n",
    "            interpreter.set_tensor(input_details[image_idx]['index'], images[i:i+1].astype(np.float32))\n",
    "            interpreter.set_tensor(input_details[stroke_idx]['index'], strokes[i:i+1].astype(np.float32))\n",
    "\n",
    "            # Run inference\n",
    "            interpreter.invoke()\n",
    "\n",
    "            # Get predictions\n",
    "            preds = interpreter.get_tensor(output_details[output_idx]['index'])\n",
    "            pred_class = np.argmax(preds, axis=1)[0]\n",
    "            true_class = np.argmax(labels[i])\n",
    "\n",
    "            if pred_class == true_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "        pbar.set_postfix(acc=f\"{correct/total:.4f}\")\n",
    "\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e4b70da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages/tensorflow/lite/python/interpreter.py:451: UserWarning:     Warning: Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite delegate for select TF ops.\n",
      "W0000 00:00:1756055891.904112  352251 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO: TfLiteFlexDelegate delegate: 8 nodes delegated out of 82 nodes with 4 partitions.\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Evaluating best_model_345_c_30000_ex_fp16_quant.tflite: 100%|█████████| 2022/2022 [3:16:48<00:00,  5.84s/it, acc=0.4744]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float16 Quantization: Size = 2.52 MB, Accuracy = 0.4744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Float16 Quantized Model\n",
    "correct_f16, total_f16 = evaluate_tflite_model('best_model_345_c_30000_ex_fp16_quant.tflite', test_ds)\n",
    "accuracy_f16 = correct_f16 / total_f16\n",
    "print(f\"Float16 Quantization: Size = {float16_size / (1024 * 1024):.2f} MB, Accuracy = {accuracy_f16:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac9cf0bc-39f3-46c9-8fe6-4f0014466ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1756067700.994469  352251 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "IOPub message rate exceeded.30000_ex_dynamic_quant.tflite:  74%|████▍ | 1499/2022 [1:29:25<31:10,  3.58s/it, acc=0.8917]\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Dynamic Range Quantized Model\n",
    "correct_dr, total_dr = evaluate_tflite_model('best_model_345_c_30000_ex_dynamic_quant.tflite', test_ds)\n",
    "accuracy_dr = correct_dr / total_dr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a88f75d3-3eff-45bb-9762-bf0841a82ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Range Quantization: Size = 1.37 MB, Accuracy = 0.8916\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dynamic Range Quantization: Size = {dynamic_range_size / (1024 * 1024):.2f} MB, Accuracy = {accuracy_dr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tens",
   "language": "python",
   "name": ".tens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

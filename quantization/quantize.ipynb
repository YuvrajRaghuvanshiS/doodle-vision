{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fa70af",
   "metadata": {},
   "source": [
    "### Theory About Reproducibility\n",
    "Reproducing or rather producing the quantized model is the deadliest pain the ass I have felt in a while. It was bad.\n",
    "\n",
    "Keeping the technicalities about SELECT_OPS and Flex Delegate etc aside, just confirm your setup as follow. I have already talked about using virtual environment as kernel in diskloader.ipynb\n",
    "\n",
    "Final setup\n",
    "- OS: Ubuntu Server  \n",
    "- Python: 3.12  \n",
    "- TensorFlow: 2.18.0 (regular, CPU-only)  \n",
    "  - Do NOT use `tensorflow[and-cuda]` (conversion fails).  \n",
    "  - Regular `tensorflow==2.18.0` works fine with Python 3.12.\n",
    "    ```bash\n",
    "    pip uninstall -y tensorflow[and-cuda]\n",
    "    pip install tensorflow==2.18.*\n",
    "    ````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dbbd92-606b-4491-a7cb-4a1154416213",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prerequisites (uncomment to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9344e68-cb7f-41e2-8787-aa60a45010eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec797e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.18.1\n",
      "Uninstalling tensorflow-2.18.1:\n",
      "  Successfully uninstalled tensorflow-2.18.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow==2.18.*\n",
      "  Using cached tensorflow-2.18.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (1.74.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (3.11.2)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.4.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorflow==2.18.*) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.18.*) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.*) (14.1.0)\n",
      "Requirement already satisfied: namex in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.*) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.*) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.*) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.*) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.*) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.*) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.*) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.*) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.*) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.*) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.*) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.*) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.*) (0.1.2)\n",
      "Using cached tensorflow-2.18.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.6 MB)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.18.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Assuming virtual environment .tens with python 3.12 is being used\n",
    "%pip uninstall -y tensorflow[and-cuda]\n",
    "%pip install tensorflow==2.18.* tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a51d2-53f4-4fcf-aa17-542fe9f0ae70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Data Preprocessing\n",
    "- No need to re-run if already ran in `diskloader.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a061a987-5ae6-465d-ad60-a61d3e60590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Download QuickDraw dataset\n",
    "# %python download_data.py\n",
    "\n",
    "# # Step 2: Combine stroke data across different partitions (train, val, test).\n",
    "# # The original data (e.g., airplanes.npz) is split into these partitions, \n",
    "# # which reduces the number of data points. We merge all partitions for a complete dataset.\n",
    "# %python combine_strokes.py\n",
    "\n",
    "# # Step 3: Convert each `.npy` and `.npz` file into individual sample files.\n",
    "# # For example, `airplanes.npy` contains (N, 784), where N is the number of samples \n",
    "# # and each sample is a flattened 28x28 image (784 features). We split this into\n",
    "# # individual `.npy` files like `airplanes/000001.npy`, `airplanes/000002.npy`, etc.\n",
    "# # This allows us to load specific slices of data (e.g., airplanes[3012:33012]) \n",
    "# # without having to load the entire file, reducing memory usage and avoiding memory eviction issues.\n",
    "# %python convert_to_individual_npys.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27283d0b-79c2-4618-90dd-4808a3056d59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de3aac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:03:39.618919: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755428619.630671  112379 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755428619.634025  112379 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81e27e",
   "metadata": {},
   "source": [
    "### Converting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbcc98ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1755428621.989813  112379 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('/home/priyanka/doodle-vision/training/best_model_345_classes_30000_examples.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab82f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbmwcyls1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbmwcyls1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpbmwcyls1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 130, 3), dtype=tf.float32, name='stroke_input'), TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='image_input')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 345), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140073260805264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260801808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260807568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260806800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260807184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260808144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260808528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260807376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260255952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260254224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260254032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260253840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260255376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260808912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260809104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260810064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260256912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260257104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260256336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260255760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260805456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260258064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260256144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260806992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260808336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260806416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260256528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260257872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260258640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260260560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260262288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260258832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260258256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260261328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260262864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260259984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260261136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260261520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260262096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260263440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260262672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140073260264016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1755428623.369812  112379 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1755428623.369833  112379 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1755428623.394882  112379 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Enable Flex ops (Select TensorFlow Ops)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS\n",
    "]\n",
    "\n",
    "# Optional optimizations\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Convert\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open('best_model_345_classes_30000_examples.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d05c7",
   "metadata": {},
   "source": [
    "### Loading and Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c95e032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyanka/doodle-vision/.tens/lib/python3.12/site-packages/tensorflow/lite/python/interpreter.py:451: UserWarning:     Warning: Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite delegate for select TF ops.\n",
      "W0000 00:00:1755428623.888249  112379 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO: TfLiteFlexDelegate delegate: 8 nodes delegated out of 61 nodes with 4 partitions.\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"best_model_345_classes_30000_examples.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44e8103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details:\n",
      "\tInput 0: name = serving_default_image_input:0, shape = [ 1 28 28  1], dtype = <class 'numpy.float32'>\n",
      "\tInput 1: name = serving_default_stroke_input:0, shape = [  1 130   3], dtype = <class 'numpy.float32'>\n",
      "Output details:\n",
      "\tInput 0: name = StatefulPartitionedCall_1:0, shape = [  1 345], dtype = <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input details:\")\n",
    "for i, d in enumerate(input_details):\n",
    "    print(f\"\\tInput {i}: name = {d['name']}, shape = {d['shape']}, dtype = {d['dtype']}\")\n",
    "\n",
    "print(\"Output details:\")\n",
    "for i, d in enumerate(output_details):\n",
    "    print(f\"\\tInput {i}: name = {d['name']}, shape = {d['shape']}, dtype = {d['dtype']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50da3803-bea7-47b0-b41c-af4004a036fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for inputs and outputs. Used when feeding the data to model later.\n",
    "image_idx = [i for i, d in enumerate(input_details) if \"image_input\" in d[\"name\"]][0]\n",
    "stroke_idx = [i for i, d in enumerate(input_details) if \"stroke_input\" in d[\"name\"]][0]\n",
    "output_idx = 0  # only one output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae6145-d1ef-4e9d-beac-bf2360a09575",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using the same dataloader pipeline as used in training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37753b84-27ce-43f2-9855-808d1caff5fb",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baf92dc8-22b9-4dd9-aeac-6ff63a379b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "LABELS_JSON    = \"/home/priyanka/doodle-vision/training/dataset/label_map.json\"\n",
    "PROCESSED_DATA_DIR = \"/home/priyanka/doodle-vision/training/dataset/processed\"\n",
    "\n",
    "# Data props\n",
    "NUM_CLASSES = 345\n",
    "\n",
    "# Training\n",
    "SAMPLES_PER_CLASS = 30_000\n",
    "SPLIT_RATIOS = (0.8, 0.1, 0.1) # train, val, test\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b1a2f93-4084-4ba9-b67b-6349d60858ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABELS_JSON, 'r') as f:\n",
    "    LABEL_MAP = json.load(f)\n",
    "    \n",
    "REV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae266a6d-6bc8-497e-bd5a-c82cac4192da",
   "metadata": {},
   "source": [
    "### Disk Data Loader for Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c19c02-dd98-42f4-a5c7-b2291b8e70de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 345/345 [19:58<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In-Memory Mode] Loaded 10,350,000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the individual + in-memory mode for tflite inference\n",
    "images = []\n",
    "strokes = []\n",
    "\n",
    "# Always list of integers\n",
    "labels = []\n",
    "\n",
    "for cls, label in tqdm(LABEL_MAP.items(), desc=\"Building dataset\"):\n",
    "    image_files = sorted(\n",
    "        glob(os.path.join(PROCESSED_DATA_DIR, \"images\", cls, \"*.npy\"))\n",
    "    )\n",
    "    stroke_files = sorted(\n",
    "        glob(os.path.join(PROCESSED_DATA_DIR, \"strokes\", cls, \"*.npy\"))\n",
    "    )\n",
    "\n",
    "    N = min(len(image_files), len(stroke_files), SAMPLES_PER_CLASS)\n",
    "\n",
    "    for i in range(N):\n",
    "        image_path = image_files[i]\n",
    "        stroke_path = stroke_files[i]\n",
    "\n",
    "        images.append(np.load(image_path))\n",
    "        strokes.append(np.load(stroke_path))\n",
    "        labels.append(label)\n",
    "\n",
    "print(f\"[In-Memory Mode] Loaded {len(labels):,} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4019aed3-1d93-4614-b567-34af9fd4f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(images, strokes, labels):\n",
    "    for image, stroke, label in zip(images, strokes, labels):\n",
    "        yield (\n",
    "            image,\n",
    "            stroke,\n",
    "            tf.one_hot(label, depth=NUM_CLASSES),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1718efeb-2084-445d-bad1-c90046bfdec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sample(img, stroke, label):\n",
    "    return {\"stroke_input\": stroke, \"image_input\": img}, label\n",
    "\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(28, 28, 1), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(130, 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(NUM_CLASSES,), dtype=tf.int32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35bea3-6aeb-4b47-b3ff-00fc780ed650",
   "metadata": {},
   "source": [
    "### Train, Val, and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4028e288-d772-416c-8b25-a99cc509b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle once and split\n",
    "total = len(labels)\n",
    "indices = np.arange(total)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_end = int(SPLIT_RATIOS[0] * total)\n",
    "val_end = train_end + int(SPLIT_RATIOS[1] * total)\n",
    "\n",
    "train_idx = indices[:train_end]\n",
    "val_idx = indices[train_end:val_end]\n",
    "test_idx = indices[val_end:]\n",
    "\n",
    "# Train\n",
    "train_images =  [images[i] for i in train_idx]\n",
    "train_strokes = [strokes[i] for i in train_idx]\n",
    "train_labels =  [labels[i] for i in train_idx]\n",
    "\n",
    "# Val\n",
    "val_images =  [images[i] for i in val_idx]\n",
    "val_strokes = [strokes[i] for i in val_idx]\n",
    "val_labels =  [labels[i] for i in val_idx]\n",
    "\n",
    "# Test\n",
    "test_images =  [images[i] for i in test_idx]\n",
    "test_strokes = [strokes[i] for i in test_idx]\n",
    "test_labels =  [labels[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a5a2932-a109-4045-bcb5-b5dee2636b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(images, strokes, labels, is_shuffle=False):\n",
    "    def gen():\n",
    "        return data_generator(images, strokes, labels)\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "    if is_shuffle:\n",
    "        ds = ds.shuffle(BATCH_SIZE * 10)\n",
    "\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.map(format_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af105553-33ab-4be4-a55a-27bea7347e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = build_dataset(test_images, test_strokes, test_labels).take(math.ceil(len(test_labels) / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac99f2e1-4d7e-44cb-8c03-613e6598f8e9",
   "metadata": {},
   "source": [
    "### Running Inference on TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e78b8a-b2c5-4edc-a74b-6818075c616a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating TFLite: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2022/2022 [2:21:26<00:00,  4.20s/it, acc=0.8916]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Create tqdm object\n",
    "pbar = tqdm(test_ds, desc=\"Evaluating TFLite\", dynamic_ncols=True)\n",
    "\n",
    "for batch in pbar:\n",
    "    # Unpack batch\n",
    "    inputs, labels = batch\n",
    "    images = inputs[\"image_input\"].numpy()\n",
    "    strokes = inputs[\"stroke_input\"].numpy()\n",
    "    labels = labels.numpy()  # one-hot vectors\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        # Set input tensors\n",
    "        interpreter.set_tensor(input_details[image_idx]['index'], images[i:i+1])\n",
    "        interpreter.set_tensor(input_details[stroke_idx]['index'], strokes[i:i+1])\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get predictions\n",
    "        preds = interpreter.get_tensor(output_details[output_idx]['index'])\n",
    "        pred_class = np.argmax(preds, axis=1)[0]\n",
    "        true_class = np.argmax(labels[i])\n",
    "\n",
    "        if pred_class == true_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    # update tqdm live with accuracy\n",
    "    pbar.set_postfix(acc=f\"{correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ec768d7-43ed-4977-946b-d1343e9e232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Test Accuracy: 0.8916 (922998/1035264)\n"
     ]
    }
   ],
   "source": [
    "# Final accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"TFLite Test Accuracy: {accuracy:.4f} ({correct}/{total})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tens",
   "language": "python",
   "name": ".tens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
